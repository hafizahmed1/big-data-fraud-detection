{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-25T17:36:46.983901Z",
     "start_time": "2024-06-25T17:36:46.971965Z"
    }
   },
   "source": "print(\"Big Data Project 6\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big Data Project 6\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T17:39:32.354718Z",
     "start_time": "2024-06-25T17:37:30.089266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from dask_ml.preprocessing import DummyEncoder, StandardScaler\n",
    "from dask_ml.impute import SimpleImputer\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from dask_ml.wrappers import ParallelPostFit\n",
    "from sklearn.metrics import classification_report\n",
    "uri=\"mongodb+srv://admin:admin@cluster0.3og2uv4.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
    "db_name='fraud_db'\n",
    "collection_name='transactions'\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()  # Starts a local scheduler and worker if no arguments are provided\n",
    "print(client)\n",
    "\n",
    "# Define the columns to load and their data types\n",
    "selected_columns = ['TransactionID', 'TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2', \n",
    "                    'card3', 'card4', 'card5', 'card6', 'dist1', 'dist2']\n",
    "\n",
    "dtypes = {\n",
    "    'TransactionID': 'int64',\n",
    "    'TransactionDT': 'int64',\n",
    "    'TransactionAmt': 'float64',\n",
    "    'ProductCD': 'object',\n",
    "    'card1': 'int64',\n",
    "    'card2': 'float64',\n",
    "    'card3': 'float64',\n",
    "    'card4': 'object',\n",
    "    'card5': 'float64',\n",
    "    'card6': 'object',\n",
    "    'dist1': 'float64',\n",
    "    'dist2': 'float64'\n",
    "}\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "df = dd.read_csv('train_transaction.csv', usecols=selected_columns + ['isFraud'], dtype=dtypes).set_index('TransactionID')\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_columns = ['TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'dist1', 'dist2']\n",
    "categorical_columns = ['ProductCD', 'card4', 'card6']\n",
    "\n",
    "# Take a small sample to fit the imputer and scaler\n",
    "sample = df.sample(frac=0.1, random_state=42).compute()\n",
    "\n",
    "# Fit the imputers\n",
    "imputer_numeric = SimpleImputer(strategy='mean').fit(sample[numeric_columns])\n",
    "imputer_categorical = SimpleImputer(strategy='most_frequent').fit(sample[categorical_columns])\n",
    "\n",
    "# Fit the scaler\n",
    "scaler = StandardScaler().fit(sample[numeric_columns])\n",
    "\n",
    "# Handle missing values for numeric columns in parallel\n",
    "df[numeric_columns] = df[numeric_columns].map_partitions(lambda df: pd.DataFrame(imputer_numeric.transform(df), columns=df.columns), meta=df[numeric_columns]._meta)\n",
    "\n",
    "# Handle missing values for categorical columns in parallel\n",
    "df[categorical_columns] = df[categorical_columns].map_partitions(lambda df: pd.DataFrame(imputer_categorical.transform(df), columns=df.columns), meta=df[categorical_columns]._meta)\n",
    "\n",
    "# Convert categorical columns to categorical dtype\n",
    "df[categorical_columns] = df[categorical_columns].categorize()\n",
    "\n",
    "# Encode categorical variables\n",
    "encoder = DummyEncoder()\n",
    "df = encoder.fit_transform(df)\n",
    "\n",
    "# Scale numeric features in parallel\n",
    "df[numeric_columns] = df[numeric_columns].map_partitions(lambda df: pd.DataFrame(scaler.transform(df), columns=df.columns), meta=df[numeric_columns]._meta)\n"
   ],
   "id": "4b8d523605ca2a63",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmad\\PycharmProjects\\big-data-fraud-detection\\.venv\\Lib\\site-packages\\distributed\\node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 53197 instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:144: UserWarning: Creating scratch directories is taking a surprisingly long time. (1.95s) This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.\n",
      "  next(self.gen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://127.0.0.1:53198' processes=4 threads=4, memory=7.92 GiB>\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T17:45:59.473227Z",
     "start_time": "2024-06-25T17:45:59.421727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dask.distributed import get_worker\n",
    "from pymongo import MongoClient\n",
    "\n",
    "def get_mongo_client():\n",
    "    worker = get_worker()  # This raises ValueError if not running within a worker context\n",
    "    if not hasattr(worker, 'mongo_client'):\n",
    "        connection_string = uri\n",
    "        worker.mongo_client = MongoClient(connection_string)\n",
    "    return worker.mongo_client\n",
    "\n",
    "def insert_into_mongo(df_part):\n",
    "    try:\n",
    "        client = get_mongo_client()\n",
    "        db = client.fraud_db\n",
    "        transactions_collection = db.transactions\n",
    "        records = df_part.to_dict(orient='records')\n",
    "        if records:\n",
    "            transactions_collection.insert_many(records)\n",
    "    except ValueError:\n",
    "        print(\"Not running on a worker. Proper MongoDB operations can't be performed.\")\n",
    "\n",
    "def load_data_from_mongo(uri, db_name, collection_name):\n",
    "    client = MongoClient(uri)\n",
    "    db = client[db_name]\n",
    "    collection = db[collection_name]\n",
    "    cursor = collection.find({})\n",
    "\n",
    "    # Convert cursor to DataFrame directly if memory allows\n",
    "    return pd.DataFrame(list(cursor))\n"
   ],
   "id": "9fa5d8551a253c55",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T17:44:03.742415Z",
     "start_time": "2024-06-25T17:42:51.643712Z"
    }
   },
   "cell_type": "code",
   "source": "df.map_partitions(insert_into_mongo, meta=int).compute()\n",
   "id": "c5b748e10d0e3e93",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmad\\PycharmProjects\\big-data-fraud-detection\\.venv\\Lib\\site-packages\\dask_expr\\_expr.py:4011: FutureWarning: Meta is not valid, `map_partitions` and `map_overlap` expects output to be a pandas object. Try passing a pandas object as meta or a dict or tuple representing the (name, dtype) of the columns. In the future the meta you passed will not work.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ahmad\\PycharmProjects\\big-data-fraud-detection\\.venv\\Lib\\site-packages\\dask_expr\\_expr.py:4011: FutureWarning: Meta is not valid, `map_partitions` and `map_overlap` expects output to be a pandas object. Try passing a pandas object as meta or a dict or tuple representing the (name, dtype) of the columns. In the future the meta you passed will not work.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_partitions\u001B[49m\u001B[43m(\u001B[49m\u001B[43minsert_into_mongo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmeta\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\big-data-fraud-detection\\.venv\\Lib\\site-packages\\dask_expr\\_collection.py:476\u001B[0m, in \u001B[0;36mFrameBase.compute\u001B[1;34m(self, fuse, **kwargs)\u001B[0m\n\u001B[0;32m    474\u001B[0m     out \u001B[38;5;241m=\u001B[39m out\u001B[38;5;241m.\u001B[39mrepartition(npartitions\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    475\u001B[0m out \u001B[38;5;241m=\u001B[39m out\u001B[38;5;241m.\u001B[39moptimize(fuse\u001B[38;5;241m=\u001B[39mfuse)\n\u001B[1;32m--> 476\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mDaskMethodsMixin\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\big-data-fraud-detection\\.venv\\Lib\\site-packages\\dask\\base.py:375\u001B[0m, in \u001B[0;36mDaskMethodsMixin.compute\u001B[1;34m(self, **kwargs)\u001B[0m\n\u001B[0;32m    351\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    352\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Compute this dask collection\u001B[39;00m\n\u001B[0;32m    353\u001B[0m \n\u001B[0;32m    354\u001B[0m \u001B[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    373\u001B[0m \u001B[38;5;124;03m    dask.compute\u001B[39;00m\n\u001B[0;32m    374\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 375\u001B[0m     (result,) \u001B[38;5;241m=\u001B[39m \u001B[43mcompute\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraverse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    376\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32m~\\PycharmProjects\\big-data-fraud-detection\\.venv\\Lib\\site-packages\\dask\\base.py:661\u001B[0m, in \u001B[0;36mcompute\u001B[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001B[0m\n\u001B[0;32m    658\u001B[0m     postcomputes\u001B[38;5;241m.\u001B[39mappend(x\u001B[38;5;241m.\u001B[39m__dask_postcompute__())\n\u001B[0;32m    660\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m shorten_traceback():\n\u001B[1;32m--> 661\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[43mschedule\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdsk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeys\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    663\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m repack([f(r, \u001B[38;5;241m*\u001B[39ma) \u001B[38;5;28;01mfor\u001B[39;00m r, (f, a) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(results, postcomputes)])\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py:655\u001B[0m, in \u001B[0;36mEvent.wait\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    653\u001B[0m signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flag\n\u001B[0;32m    654\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m signaled:\n\u001B[1;32m--> 655\u001B[0m     signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cond\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    656\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m signaled\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py:359\u001B[0m, in \u001B[0;36mCondition.wait\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    357\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    358\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m--> 359\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    360\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    361\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m waiter\u001B[38;5;241m.\u001B[39macquire(\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Fetch Data from MongoDB\n",
    "#from dask.delayed import delayed\n",
    "#chunks = load_data_from_mongo(uri, db_name, collection_name, chunk_size=10000)\n",
    "\n",
    "# Create Dask DataFrame from delayed chunks\n",
    "#ddf = dd.from_delayed([delayed(pd.DataFrame)(chunk) for chunk in chunks])"
   ],
   "id": "6abd5e76d6a1048a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T17:40:05.967881Z",
     "start_time": "2024-06-25T17:39:44.364993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert to Dask array\n",
    "X = df.drop('isFraud', axis=1).to_dask_array(lengths=True)\n",
    "y = df['isFraud'].to_dask_array(lengths=True)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Initialize the model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Use ParallelPostFit to work with Dask\n",
    "model = ParallelPostFit(estimator=rf)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Compute the classification report\n",
    "report = classification_report(y_test.compute(), y_pred.compute())\n",
    "\n",
    "print(report)"
   ],
   "id": "928ef7f96fc66bf",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Convert to Dask array\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdrop\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43misFraud\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_dask_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlengths\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m y \u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124misFraud\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto_dask_array(lengths\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Train-test split\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\big-data-fraud-detection\\.venv\\Lib\\site-packages\\dask_expr\\_collection.py:1416\u001B[0m, in \u001B[0;36mFrameBase.to_dask_array\u001B[1;34m(self, lengths, meta, optimize, **optimize_kwargs)\u001B[0m\n\u001B[0;32m   1388\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mto_dask_array\u001B[39m(\n\u001B[0;32m   1389\u001B[0m     \u001B[38;5;28mself\u001B[39m, lengths\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, meta\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, optimize: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptimize_kwargs\n\u001B[0;32m   1390\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Array:\n\u001B[0;32m   1391\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Convert a dask DataFrame to a dask array.\u001B[39;00m\n\u001B[0;32m   1392\u001B[0m \n\u001B[0;32m   1393\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1414\u001B[0m \u001B[38;5;124;03m    A Dask Array\u001B[39;00m\n\u001B[0;32m   1415\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1416\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_legacy_dataframe\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptimize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43moptimize_kwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_dask_array\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1417\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlengths\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlengths\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmeta\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmeta\u001B[49m\n\u001B[0;32m   1418\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\big-data-fraud-detection\\.venv\\Lib\\site-packages\\dask\\dataframe\\core.py:2045\u001B[0m, in \u001B[0;36m_Frame.to_dask_array\u001B[1;34m(self, lengths, meta)\u001B[0m\n\u001B[0;32m   2022\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Convert a dask DataFrame to a dask array.\u001B[39;00m\n\u001B[0;32m   2023\u001B[0m \n\u001B[0;32m   2024\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2042\u001B[0m \u001B[38;5;124;03m-------\u001B[39;00m\n\u001B[0;32m   2043\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   2044\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m lengths \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m-> 2045\u001B[0m     lengths \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_partitions\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menforce_metadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m   2047\u001B[0m arr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalues\n\u001B[0;32m   2049\u001B[0m chunks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_chunks(arr, lengths)\n",
      "File \u001B[1;32m~\\PycharmProjects\\big-data-fraud-detection\\.venv\\Lib\\site-packages\\dask\\base.py:375\u001B[0m, in \u001B[0;36mDaskMethodsMixin.compute\u001B[1;34m(self, **kwargs)\u001B[0m\n\u001B[0;32m    351\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    352\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Compute this dask collection\u001B[39;00m\n\u001B[0;32m    353\u001B[0m \n\u001B[0;32m    354\u001B[0m \u001B[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    373\u001B[0m \u001B[38;5;124;03m    dask.compute\u001B[39;00m\n\u001B[0;32m    374\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 375\u001B[0m     (result,) \u001B[38;5;241m=\u001B[39m \u001B[43mcompute\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraverse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    376\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32m~\\PycharmProjects\\big-data-fraud-detection\\.venv\\Lib\\site-packages\\dask\\base.py:661\u001B[0m, in \u001B[0;36mcompute\u001B[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001B[0m\n\u001B[0;32m    658\u001B[0m     postcomputes\u001B[38;5;241m.\u001B[39mappend(x\u001B[38;5;241m.\u001B[39m__dask_postcompute__())\n\u001B[0;32m    660\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m shorten_traceback():\n\u001B[1;32m--> 661\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[43mschedule\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdsk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeys\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    663\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m repack([f(r, \u001B[38;5;241m*\u001B[39ma) \u001B[38;5;28;01mfor\u001B[39;00m r, (f, a) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(results, postcomputes)])\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py:655\u001B[0m, in \u001B[0;36mEvent.wait\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    653\u001B[0m signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flag\n\u001B[0;32m    654\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m signaled:\n\u001B[1;32m--> 655\u001B[0m     signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cond\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    656\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m signaled\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py:359\u001B[0m, in \u001B[0;36mCondition.wait\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    357\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    358\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m--> 359\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    360\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    361\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m waiter\u001B[38;5;241m.\u001B[39macquire(\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T06:00:50.964387Z",
     "start_time": "2024-06-25T06:00:50.866582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Predict isFraud on test dataset\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Load the test dataset\n",
    "test_df = dd.read_csv('test_transaction.csv', usecols=['TransactionID'] + selected_columns[1:], dtype=dtypes).set_index('TransactionID')\n",
    "\n",
    "print(test_df.columns)"
   ],
   "id": "a262567b9bee542b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2',\n",
      "       'card3', 'card4', 'card5', 'card6', 'dist1', 'dist2'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T06:01:04.156071Z",
     "start_time": "2024-06-25T06:00:50.966234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Identify numeric and categorical columns\n",
    "numeric_columns_test = ['TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'dist1', 'dist2']\n",
    "categorical_columns_test = ['ProductCD', 'card4', 'card6']\n",
    "\n",
    "# Handle missing values for numeric columns\n",
    "test_df[numeric_columns_test] = test_df[numeric_columns_test].map_partitions(lambda df: imputer_numeric.fit_transform(df), meta=test_df[numeric_columns_test]._meta)\n",
    "\n",
    "# Handle missing values for categorical columns\n",
    "test_df[categorical_columns_test] = test_df[categorical_columns_test].map_partitions(lambda df: imputer_categorical.fit_transform(df), meta=test_df[categorical_columns_test]._meta)\n",
    "\n",
    "# Convert categorical columns to categorical dtype\n",
    "test_df[categorical_columns_test] = test_df[categorical_columns_test].categorize()\n",
    "\n",
    "# Encode categorical variables\n",
    "encoder_test = DummyEncoder()\n",
    "test_df = encoder_test.fit_transform(test_df)\n",
    "\n",
    "# Take a small sample to fit the scaler\n",
    "sample_test = test_df.sample(frac=0.1, random_state=42).compute()\n",
    "\n",
    "# Fit the scaler on the sample\n",
    "scaler_test = StandardScaler().fit(sample_test[numeric_columns_test])\n",
    "\n",
    "# Scale numeric features in parallel\n",
    "test_df[numeric_columns_test] = test_df[numeric_columns_test].map_partitions(lambda df: pd.DataFrame(scaler_test.transform(df), columns=df.columns), meta=test_df[numeric_columns_test]._meta)\n",
    "\n",
    "# Ensure the missing 'card6_debit or credit' column is added with zeros\n",
    "if 'card6_debit or credit' not in test_df.columns:\n",
    "    test_df['card6_debit or credit'] = 0\n"
   ],
   "id": "5a48e813191dcfac",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T06:01:04.167689Z",
     "start_time": "2024-06-25T06:01:04.156071Z"
    }
   },
   "cell_type": "code",
   "source": "print(test_df.columns)",
   "id": "553f54c876a6181a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5',\n",
      "       'dist1', 'dist2', 'ProductCD_C', 'ProductCD_H', 'ProductCD_R',\n",
      "       'ProductCD_S', 'ProductCD_W', 'card4_american express',\n",
      "       'card4_discover', 'card4_mastercard', 'card4_visa', 'card6_charge card',\n",
      "       'card6_credit', 'card6_debit', 'card6_debit or credit'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T06:01:27.185756Z",
     "start_time": "2024-06-25T06:01:04.170376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Predict isFraud on the test dataset\n",
    "test_predictions = model.predict(test_df)\n",
    "\n",
    "# Define the path to save the new submission file\n",
    "submission_file_path = r'C:\\Users\\SvenEggers\\.kaggle\\sample_submission_1.csv'\n",
    "\n",
    "# Load the sample submission file to get the TransactionID\n",
    "submission_df = dd.read_csv('sample_submission.csv').set_index('TransactionID')\n",
    "\n",
    "# Convert predictions to a Dask DataFrame and align with the submission DataFrame\n",
    "predictions_df = dd.from_pandas(pd.DataFrame({\n",
    "    'TransactionID': test_df.index.compute(),\n",
    "    'isFraud': test_predictions.compute()\n",
    "}), npartitions=1).set_index('TransactionID')\n",
    "\n",
    "# Merge the predictions with the submission file\n",
    "submission_df = submission_df.drop(columns='isFraud', errors='ignore')  # Drop existing isFraud if present\n",
    "submission_df = submission_df.merge(predictions_df, left_index=True, right_index=True, how='left')\n",
    "\n",
    "# Save the updated submission file, replacing if it exists\n",
    "submission_df.to_csv(submission_file_path, single_file=True)\n",
    "\n",
    "print(f'Submission file saved to {submission_file_path}')"
   ],
   "id": "3ee1147f752b2f98",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SvenEggers\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\SvenEggers\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved to C:\\Users\\SvenEggers\\.kaggle\\sample_submission_1.csv\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T06:01:27.413898Z",
     "start_time": "2024-06-25T06:01:27.185756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Count the numbers of isFraud in the submission\n",
    "\n",
    "# Define the path to the sample submission dataset\n",
    "sample_submission_df = pd.read_csv('sample_submission_1.csv')\n",
    "\n",
    "# Check the first few rows to ensure it loaded correctly\n",
    "print(sample_submission_df.head())\n",
    "\n",
    "# Count the number of 1s in the 'isFraud' column\n",
    "is_fraud_count = sample_submission_df['isFraud'].sum()\n",
    "\n",
    "print(f\"The number of 1s in the 'isFraud' column: {is_fraud_count}\")"
   ],
   "id": "c10df3b9437a409f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TransactionID  isFraud\n",
      "0        3663549        0\n",
      "1        3663550        0\n",
      "2        3663551        0\n",
      "3        3663552        0\n",
      "4        3663553        0\n",
      "The number of 1s in the 'isFraud' column: 1148\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cfe73e75df9199b6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

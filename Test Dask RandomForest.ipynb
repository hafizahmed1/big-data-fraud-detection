{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": "print(\"Big Data Project 6\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T16:06:29.402609Z",
     "start_time": "2024-06-08T16:06:21.290247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from dask_ml.preprocessing import DummyEncoder, StandardScaler\n",
    "from dask_ml.impute import SimpleImputer\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from dask_ml.wrappers import ParallelPostFit\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define the columns to load and their data types\n",
    "selected_columns = ['TransactionID', 'TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2', \n",
    "                    'card3', 'card4', 'card5', 'card6', 'dist1', 'dist2']\n",
    "\n",
    "dtypes = {\n",
    "    'TransactionID': 'int64',\n",
    "    'TransactionDT': 'int64',\n",
    "    'TransactionAmt': 'float64',\n",
    "    'ProductCD': 'object',\n",
    "    'card1': 'int64',\n",
    "    'card2': 'float64',\n",
    "    'card3': 'float64',\n",
    "    'card4': 'object',\n",
    "    'card5': 'float64',\n",
    "    'card6': 'object',\n",
    "    'dist1': 'float64',\n",
    "    'dist2': 'float64'\n",
    "}\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "df = dd.read_csv(r'C:\\Users\\SvenEggers\\.kaggle\\train_transaction.csv', usecols=selected_columns + ['isFraud'], dtype=dtypes).set_index('TransactionID')\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_columns = ['TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'dist1', 'dist2']\n",
    "categorical_columns = ['ProductCD', 'card4', 'card6']\n",
    "\n",
    "# Handle missing values for numeric columns\n",
    "imputer_numeric = SimpleImputer(strategy='mean')\n",
    "df[numeric_columns] = imputer_numeric.fit_transform(df[numeric_columns])\n",
    "\n",
    "# Handle missing values for categorical columns\n",
    "imputer_categorical = SimpleImputer(strategy='most_frequent')\n",
    "df[categorical_columns] = imputer_categorical.fit_transform(df[categorical_columns])\n",
    "\n",
    "# Convert categorical columns to categorical dtype\n",
    "df[categorical_columns] = df[categorical_columns].categorize()\n",
    "\n",
    "# Encode categorical variables\n",
    "encoder = DummyEncoder()\n",
    "df = encoder.fit_transform(df)\n",
    "\n",
    "# Scale numeric features\n",
    "scaler = StandardScaler()\n",
    "df[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n"
   ],
   "id": "4b8d523605ca2a63",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T16:06:32.118220Z",
     "start_time": "2024-06-08T16:06:32.100761Z"
    }
   },
   "cell_type": "code",
   "source": "print(df.columns)",
   "id": "71ce77807b8d753b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['isFraud', 'TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3',\n",
      "       'card5', 'dist1', 'dist2', 'ProductCD_C', 'ProductCD_H', 'ProductCD_R',\n",
      "       'ProductCD_S', 'ProductCD_W', 'card4_american express',\n",
      "       'card4_discover', 'card4_mastercard', 'card4_visa', 'card6_charge card',\n",
      "       'card6_credit', 'card6_debit', 'card6_debit or credit'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T16:07:48.885595Z",
     "start_time": "2024-06-08T16:06:34.679002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = df.drop('isFraud', axis=1)\n",
    "y = df['isFraud']\n",
    "\n",
    "# Convert to Dask array\n",
    "X = X.to_dask_array(lengths=True)\n",
    "y = y.to_dask_array(lengths=True)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Initialize the model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Use ParallelPostFit to work with Dask\n",
    "model = ParallelPostFit(estimator=rf)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Compute the classification report\n",
    "report = classification_report(y_test.compute(), y_pred.compute())\n",
    "\n",
    "print(report)"
   ],
   "id": "928ef7f96fc66bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99    113931\n",
      "           1       0.84      0.45      0.58      4182\n",
      "\n",
      "    accuracy                           0.98    118113\n",
      "   macro avg       0.91      0.72      0.79    118113\n",
      "weighted avg       0.98      0.98      0.97    118113\n",
      "\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T14:12:23.480180Z",
     "start_time": "2024-06-08T14:12:23.447329Z"
    }
   },
   "cell_type": "code",
   "source": "print(X.columns)",
   "id": "74739dc41295ac02",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Array' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[35], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mX\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'Array' object has no attribute 'columns'"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T16:07:54.848062Z",
     "start_time": "2024-06-08T16:07:54.831889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Define the path to the test dataset\n",
    "test_dataset_path = r'C:\\Users\\SvenEggers\\.kaggle\\test_transaction.csv'\n",
    "\n",
    "# Load the test dataset\n",
    "test_df = dd.read_csv(test_dataset_path, usecols=['TransactionID'] + selected_columns[1:], dtype=dtypes).set_index('TransactionID')\n",
    "\n",
    "print(test_df.columns)"
   ],
   "id": "a262567b9bee542b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2',\n",
      "       'card3', 'card4', 'card5', 'card6', 'dist1', 'dist2'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T16:08:04.770207Z",
     "start_time": "2024-06-08T16:07:57.586950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Identify numeric and categorical columns\n",
    "numeric_columns_test = ['TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'dist1', 'dist2']\n",
    "categorical_columns_test = ['ProductCD', 'card4', 'card6']\n",
    "\n",
    "# Handle missing values for numeric columns\n",
    "imputer_numeric_test = SimpleImputer(strategy='mean')\n",
    "test_df[numeric_columns_test] = imputer_numeric_test.fit_transform(test_df[numeric_columns_test])\n",
    "\n",
    "# Handle missing values for categorical columns\n",
    "imputer_categorical_test = SimpleImputer(strategy='most_frequent')\n",
    "test_df[categorical_columns_test] = imputer_categorical_test.fit_transform(test_df[categorical_columns_test])\n",
    "\n",
    "# Convert categorical columns to categorical dtype\n",
    "test_df[categorical_columns_test] = test_df[categorical_columns_test].categorize()\n",
    "\n",
    "# Encode categorical variables\n",
    "encoder_test = DummyEncoder()\n",
    "test_df = encoder_test.fit_transform(test_df)\n",
    "\n",
    "# Scale numeric features\n",
    "scaler_test = StandardScaler()\n",
    "test_df[numeric_columns_test] = scaler_test.fit_transform(test_df[numeric_columns_test])\n",
    "\n",
    "# Ensure the missing 'card6_debit or credit' column is added with zeros\n",
    "if 'card6_debit or credit' not in test_df.columns:\n",
    "    test_df['card6_debit or credit'] = 0\n",
    "\n",
    "\n"
   ],
   "id": "5a48e813191dcfac",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T16:08:07.929554Z",
     "start_time": "2024-06-08T16:08:07.917964Z"
    }
   },
   "cell_type": "code",
   "source": "print(test_df.columns)",
   "id": "553f54c876a6181a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5',\n",
      "       'dist1', 'dist2', 'ProductCD_C', 'ProductCD_H', 'ProductCD_R',\n",
      "       'ProductCD_S', 'ProductCD_W', 'card4_american express',\n",
      "       'card4_discover', 'card4_mastercard', 'card4_visa', 'card6_charge card',\n",
      "       'card6_credit', 'card6_debit', 'card6_debit or credit'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T16:08:33.786226Z",
     "start_time": "2024-06-08T16:08:24.814359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#------------------------------------------\n",
    "\n",
    "# Predict isFraud on the test dataset\n",
    "test_predictions = model.predict(test_df)\n",
    "\n",
    "\n",
    "# Define the path to save the new submission file\n",
    "submission_file_path = r'C:\\Users\\SvenEggers\\.kaggle\\sample_submission_1.csv'\n",
    "\n",
    "# Load the sample submission file to get the TransactionID\n",
    "submission_df = dd.read_csv(r'C:\\Users\\SvenEggers\\.kaggle\\sample_submission.csv')\n",
    "\n",
    "# Ensure the submission_df is indexed by TransactionID to align with predictions\n",
    "submission_df = submission_df.set_index('TransactionID')\n",
    "\n",
    "# Convert predictions to a Dask DataFrame and align with the submission DataFrame\n",
    "predictions_df = dd.from_pandas(pd.DataFrame({\n",
    "    'TransactionID': test_df.index.compute(),\n",
    "    'isFraud': test_predictions.compute()\n",
    "}), npartitions=1).set_index('TransactionID')\n",
    "\n",
    "# Merge the predictions with the submission file\n",
    "submission_df = submission_df.drop(columns='isFraud', errors='ignore')  # Drop existing isFraud if present\n",
    "submission_df = submission_df.merge(predictions_df, left_index=True, right_index=True, how='left')\n",
    "\n",
    "# Save the updated submission file, replacing if it exists\n",
    "submission_df.to_csv(submission_file_path, single_file=True)\n",
    "\n",
    "print(f'Submission file saved to {submission_file_path}')"
   ],
   "id": "3ee1147f752b2f98",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SvenEggers\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\SvenEggers\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved to C:\\Users\\SvenEggers\\.kaggle\\sample_submission_1.csv\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T16:13:17.422104Z",
     "start_time": "2024-06-08T16:13:17.333784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Count the numbers of isFraud in the submission\n",
    "\n",
    "# Define the path to the sample submission dataset\n",
    "sample_submission_path = r'C:\\Users\\SvenEggers\\.kaggle\\sample_submission_1.csv'\n",
    "\n",
    "# Load the sample submission dataset\n",
    "sample_submission_df = pd.read_csv(sample_submission_path)\n",
    "\n",
    "# Check the first few rows to ensure it loaded correctly\n",
    "print(sample_submission_df.head())\n",
    "\n",
    "# Count the number of 1s in the 'isFraud' column\n",
    "is_fraud_count = sample_submission_df['isFraud'].sum()\n",
    "\n",
    "print(f\"The number of 1s in the 'isFraud' column: {is_fraud_count}\")"
   ],
   "id": "c10df3b9437a409f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TransactionID  isFraud\n",
      "0        3663549        0\n",
      "1        3663550        0\n",
      "2        3663551        0\n",
      "3        3663552        0\n",
      "4        3663553        0\n",
      "The number of 1s in the 'isFraud' column: 1517\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cfe73e75df9199b6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

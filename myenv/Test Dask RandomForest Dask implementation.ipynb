{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-14T16:55:14.844734Z",
     "start_time": "2024-06-14T16:55:14.837680Z"
    }
   },
   "source": "print(\"Big Data Project 6\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big Data Project 6\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T18:02:21.457225Z",
     "start_time": "2024-06-14T18:02:17.548590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from dask_ml.preprocessing import DummyEncoder, StandardScaler\n",
    "from dask_ml.impute import SimpleImputer\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from dask_ml.wrappers import ParallelPostFit\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define the columns to load and their data types\n",
    "selected_columns = ['TransactionID', 'TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2', \n",
    "                    'card3', 'card4', 'card5', 'card6', 'dist1', 'dist2']\n",
    "\n",
    "dtypes = {\n",
    "    'TransactionID': 'int64',\n",
    "    'TransactionDT': 'int64',\n",
    "    'TransactionAmt': 'float64',\n",
    "    'ProductCD': 'object',\n",
    "    'card1': 'int64',\n",
    "    'card2': 'float64',\n",
    "    'card3': 'float64',\n",
    "    'card4': 'object',\n",
    "    'card5': 'float64',\n",
    "    'card6': 'object',\n",
    "    'dist1': 'float64',\n",
    "    'dist2': 'float64'\n",
    "}\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "df = dd.read_csv(r'C:\\Users\\SvenEggers\\.kaggle\\train_transaction.csv', usecols=selected_columns + ['isFraud'], dtype=dtypes).set_index('TransactionID')\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_columns = ['TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'dist1', 'dist2']\n",
    "categorical_columns = ['ProductCD', 'card4', 'card6']\n",
    "\n",
    "# Take a small sample to fit the imputer and scaler\n",
    "sample = df.sample(frac=0.1, random_state=42).compute()\n",
    "\n",
    "# Fit the imputers\n",
    "imputer_numeric = SimpleImputer(strategy='mean').fit(sample[numeric_columns])\n",
    "imputer_categorical = SimpleImputer(strategy='most_frequent').fit(sample[categorical_columns])\n",
    "\n",
    "# Fit the scaler\n",
    "scaler = StandardScaler().fit(sample[numeric_columns])\n",
    "\n",
    "# Handle missing values for numeric columns in parallel\n",
    "df[numeric_columns] = df[numeric_columns].map_partitions(lambda df: pd.DataFrame(imputer_numeric.transform(df), columns=df.columns), meta=df[numeric_columns]._meta)\n",
    "\n",
    "# Handle missing values for categorical columns in parallel\n",
    "df[categorical_columns] = df[categorical_columns].map_partitions(lambda df: pd.DataFrame(imputer_categorical.transform(df), columns=df.columns), meta=df[categorical_columns]._meta)\n",
    "\n",
    "# Convert categorical columns to categorical dtype\n",
    "df[categorical_columns] = df[categorical_columns].categorize()\n",
    "\n",
    "# Encode categorical variables\n",
    "encoder = DummyEncoder()\n",
    "df = encoder.fit_transform(df)\n",
    "\n",
    "# Scale numeric features in parallel\n",
    "df[numeric_columns] = df[numeric_columns].map_partitions(lambda df: pd.DataFrame(scaler.transform(df), columns=df.columns), meta=df[numeric_columns]._meta)\n"
   ],
   "id": "4b8d523605ca2a63",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T17:11:08.318249Z",
     "start_time": "2024-06-14T17:11:08.299429Z"
    }
   },
   "cell_type": "code",
   "source": "print(df.columns)",
   "id": "71ce77807b8d753b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['isFraud', 'TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3',\n",
      "       'card5', 'dist1', 'dist2', 'ProductCD_C', 'ProductCD_H', 'ProductCD_R',\n",
      "       'ProductCD_S', 'ProductCD_W', 'card4_american express',\n",
      "       'card4_discover', 'card4_mastercard', 'card4_visa', 'card6_charge card',\n",
      "       'card6_credit', 'card6_debit', 'card6_debit or credit'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T18:03:35.080898Z",
     "start_time": "2024-06-14T18:03:35.035625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert to Dask array\n",
    "X = df.drop('isFraud', axis=1).to_dask_array(lengths=True)\n",
    "y = df['isFraud'].to_dask_array(lengths=True)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Initialize the model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Use ParallelPostFit to work with Dask\n",
    "model = ParallelPostFit(estimator=rf)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Compute the classification report\n",
    "report = classification_report(y_test.compute(), y_pred.compute())\n",
    "\n",
    "print(report)"
   ],
   "id": "928ef7f96fc66bf",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "FrameBase.optimize() got an unexpected keyword argument 'chunks'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[28], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Convert to Dask array\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdrop\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43misFraud\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_dask_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlengths\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m100000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m y \u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124misFraud\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto_dask_array(lengths\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, chunks\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m100000\u001B[39m,))\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Train-test split\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dask_expr\\_collection.py:1416\u001B[0m, in \u001B[0;36mFrameBase.to_dask_array\u001B[1;34m(self, lengths, meta, optimize, **optimize_kwargs)\u001B[0m\n\u001B[0;32m   1388\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mto_dask_array\u001B[39m(\n\u001B[0;32m   1389\u001B[0m     \u001B[38;5;28mself\u001B[39m, lengths\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, meta\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, optimize: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptimize_kwargs\n\u001B[0;32m   1390\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Array:\n\u001B[0;32m   1391\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Convert a dask DataFrame to a dask array.\u001B[39;00m\n\u001B[0;32m   1392\u001B[0m \n\u001B[0;32m   1393\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1414\u001B[0m \u001B[38;5;124;03m    A Dask Array\u001B[39;00m\n\u001B[0;32m   1415\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1416\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_legacy_dataframe\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptimize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43moptimize_kwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mto_dask_array(\n\u001B[0;32m   1417\u001B[0m         lengths\u001B[38;5;241m=\u001B[39mlengths, meta\u001B[38;5;241m=\u001B[39mmeta\n\u001B[0;32m   1418\u001B[0m     )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dask_expr\\_collection.py:1385\u001B[0m, in \u001B[0;36mFrameBase.to_legacy_dataframe\u001B[1;34m(self, optimize, **optimize_kwargs)\u001B[0m\n\u001B[0;32m   1375\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mto_legacy_dataframe\u001B[39m(\u001B[38;5;28mself\u001B[39m, optimize: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptimize_kwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m _Frame:\n\u001B[0;32m   1376\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Convert to a legacy dask-dataframe collection\u001B[39;00m\n\u001B[0;32m   1377\u001B[0m \n\u001B[0;32m   1378\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1383\u001B[0m \u001B[38;5;124;03m        Key-word arguments to pass through to `optimize`.\u001B[39;00m\n\u001B[0;32m   1384\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1385\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimize\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43moptimize_kwargs\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m optimize \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n\u001B[0;32m   1386\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m new_dd_object(df\u001B[38;5;241m.\u001B[39mdask, df\u001B[38;5;241m.\u001B[39m_name, df\u001B[38;5;241m.\u001B[39m_meta, df\u001B[38;5;241m.\u001B[39mdivisions)\n",
      "\u001B[1;31mTypeError\u001B[0m: FrameBase.optimize() got an unexpected keyword argument 'chunks'"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T17:28:25.238461Z",
     "start_time": "2024-06-14T17:28:25.223090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Predict isFraud on test dataset\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Define the path to the test dataset\n",
    "test_dataset_path = r'C:\\Users\\SvenEggers\\.kaggle\\test_transaction.csv'\n",
    "\n",
    "# Load the test dataset\n",
    "test_df = dd.read_csv(test_dataset_path, usecols=['TransactionID'] + selected_columns[1:], dtype=dtypes).set_index('TransactionID')\n",
    "\n",
    "print(test_df.columns)"
   ],
   "id": "a262567b9bee542b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2',\n",
      "       'card3', 'card4', 'card5', 'card6', 'dist1', 'dist2'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T17:28:34.825947Z",
     "start_time": "2024-06-14T17:28:30.716417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Identify numeric and categorical columns\n",
    "numeric_columns_test = ['TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'dist1', 'dist2']\n",
    "categorical_columns_test = ['ProductCD', 'card4', 'card6']\n",
    "\n",
    "# Handle missing values for numeric columns\n",
    "test_df[numeric_columns_test] = test_df[numeric_columns_test].map_partitions(lambda df: imputer_numeric.fit_transform(df), meta=test_df[numeric_columns_test]._meta)\n",
    "\n",
    "# Handle missing values for categorical columns\n",
    "test_df[categorical_columns_test] = test_df[categorical_columns_test].map_partitions(lambda df: imputer_categorical.fit_transform(df), meta=test_df[categorical_columns_test]._meta)\n",
    "\n",
    "# Convert categorical columns to categorical dtype\n",
    "test_df[categorical_columns_test] = test_df[categorical_columns_test].categorize()\n",
    "\n",
    "# Encode categorical variables\n",
    "encoder_test = DummyEncoder()\n",
    "test_df = encoder_test.fit_transform(test_df)\n",
    "\n",
    "# Take a small sample to fit the scaler\n",
    "sample_test = test_df.sample(frac=0.1, random_state=42).compute()\n",
    "\n",
    "# Fit the scaler on the sample\n",
    "scaler_test = StandardScaler().fit(sample_test[numeric_columns_test])\n",
    "\n",
    "# Scale numeric features in parallel\n",
    "test_df[numeric_columns_test] = test_df[numeric_columns_test].map_partitions(lambda df: pd.DataFrame(scaler_test.transform(df), columns=df.columns), meta=test_df[numeric_columns_test]._meta)\n",
    "\n",
    "# Ensure the missing 'card6_debit or credit' column is added with zeros\n",
    "if 'card6_debit or credit' not in test_df.columns:\n",
    "    test_df['card6_debit or credit'] = 0\n"
   ],
   "id": "5a48e813191dcfac",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T17:20:29.833338Z",
     "start_time": "2024-06-14T17:20:29.818372Z"
    }
   },
   "cell_type": "code",
   "source": "print(test_df.columns)",
   "id": "553f54c876a6181a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5',\n",
      "       'dist1', 'dist2', 'ProductCD_C', 'ProductCD_H', 'ProductCD_R',\n",
      "       'ProductCD_S', 'ProductCD_W', 'card4_american express',\n",
      "       'card4_discover', 'card4_mastercard', 'card4_visa', 'card6_charge card',\n",
      "       'card6_credit', 'card6_debit', 'card6_debit or credit'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T17:29:49.565035Z",
     "start_time": "2024-06-14T17:29:40.895052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Predict isFraud on the test dataset\n",
    "test_predictions = model.predict(test_df)\n",
    "\n",
    "\n",
    "# Define the path to save the new submission file\n",
    "submission_file_path = r'C:\\Users\\SvenEggers\\.kaggle\\sample_submission_1.csv'\n",
    "\n",
    "# Load the sample submission file to get the TransactionID\n",
    "submission_df = dd.read_csv(r'C:\\Users\\SvenEggers\\.kaggle\\sample_submission.csv')\n",
    "\n",
    "# Ensure the submission_df is indexed by TransactionID to align with predictions\n",
    "submission_df = submission_df.set_index('TransactionID')\n",
    "\n",
    "# Convert predictions to a Dask DataFrame and align with the submission DataFrame\n",
    "predictions_df = dd.from_pandas(pd.DataFrame({\n",
    "    'TransactionID': test_df.index.compute(),\n",
    "    'isFraud': test_predictions.compute()\n",
    "}), npartitions=1).set_index('TransactionID')\n",
    "\n",
    "# Merge the predictions with the submission file\n",
    "submission_df = submission_df.drop(columns='isFraud', errors='ignore')  # Drop existing isFraud if present\n",
    "submission_df = submission_df.merge(predictions_df, left_index=True, right_index=True, how='left')\n",
    "\n",
    "# Save the updated submission file, replacing if it exists\n",
    "submission_df.to_csv(submission_file_path, single_file=True)\n",
    "\n",
    "print(f'Submission file saved to {submission_file_path}')"
   ],
   "id": "3ee1147f752b2f98",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SvenEggers\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\SvenEggers\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved to C:\\Users\\SvenEggers\\.kaggle\\sample_submission_1.csv\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T17:30:08.012713Z",
     "start_time": "2024-06-14T17:30:07.929600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Count the numbers of isFraud in the submission\n",
    "\n",
    "# Define the path to the sample submission dataset\n",
    "sample_submission_path = r'C:\\Users\\SvenEggers\\.kaggle\\sample_submission_1.csv'\n",
    "\n",
    "# Load the sample submission dataset\n",
    "sample_submission_df = pd.read_csv(sample_submission_path)\n",
    "\n",
    "# Check the first few rows to ensure it loaded correctly\n",
    "print(sample_submission_df.head())\n",
    "\n",
    "# Count the number of 1s in the 'isFraud' column\n",
    "is_fraud_count = sample_submission_df['isFraud'].sum()\n",
    "\n",
    "print(f\"The number of 1s in the 'isFraud' column: {is_fraud_count}\")"
   ],
   "id": "c10df3b9437a409f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TransactionID  isFraud\n",
      "0        3663549        0\n",
      "1        3663550        0\n",
      "2        3663551        0\n",
      "3        3663552        0\n",
      "4        3663553        0\n",
      "The number of 1s in the 'isFraud' column: 1148\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cfe73e75df9199b6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

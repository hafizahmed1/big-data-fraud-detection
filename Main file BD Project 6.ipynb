{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-25T18:58:40.483779Z",
     "start_time": "2024-06-25T18:58:40.463806Z"
    }
   },
   "source": "print(\"Big Data Project 6\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big Data Project 6\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T18:59:17.671980Z",
     "start_time": "2024-06-25T18:58:44.344464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from dask_ml.preprocessing import DummyEncoder, StandardScaler\n",
    "from dask_ml.impute import SimpleImputer\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from dask_ml.wrappers import ParallelPostFit\n",
    "from sklearn.metrics import classification_report\n",
    "uri=\"mongodb+srv://admin:admin@cluster0.3og2uv4.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
    "db_name='fraud_db'\n",
    "collection_name='transactions'\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()  # Starts a local scheduler and worker if no arguments are provided\n",
    "print(client)\n",
    "\n",
    "# Define the columns to load and their data types\n",
    "selected_columns = ['TransactionID', 'TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2', \n",
    "                    'card3', 'card4', 'card5', 'card6', 'dist1', 'dist2']\n",
    "\n",
    "dtypes = {\n",
    "    'TransactionID': 'int64',\n",
    "    'TransactionDT': 'int64',\n",
    "    'TransactionAmt': 'float64',\n",
    "    'ProductCD': 'object',\n",
    "    'card1': 'int64',\n",
    "    'card2': 'float64',\n",
    "    'card3': 'float64',\n",
    "    'card4': 'object',\n",
    "    'card5': 'float64',\n",
    "    'card6': 'object',\n",
    "    'dist1': 'float64',\n",
    "    'dist2': 'float64'\n",
    "}\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "df = dd.read_csv('train_transaction.csv', usecols=selected_columns + ['isFraud'], dtype=dtypes).set_index('TransactionID')\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_columns = ['TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'dist1', 'dist2']\n",
    "categorical_columns = ['ProductCD', 'card4', 'card6']\n",
    "\n",
    "# Take a small sample to fit the imputer and scaler\n",
    "sample = df.sample(frac=0.1, random_state=42).compute()\n",
    "\n",
    "# Fit the imputers\n",
    "imputer_numeric = SimpleImputer(strategy='mean').fit(sample[numeric_columns])\n",
    "imputer_categorical = SimpleImputer(strategy='most_frequent').fit(sample[categorical_columns])\n",
    "\n",
    "# Fit the scaler\n",
    "scaler = StandardScaler().fit(sample[numeric_columns])\n",
    "\n",
    "# Handle missing values for numeric columns in parallel\n",
    "df[numeric_columns] = df[numeric_columns].map_partitions(lambda df: pd.DataFrame(imputer_numeric.transform(df), columns=df.columns), meta=df[numeric_columns]._meta)\n",
    "\n",
    "# Handle missing values for categorical columns in parallel\n",
    "df[categorical_columns] = df[categorical_columns].map_partitions(lambda df: pd.DataFrame(imputer_categorical.transform(df), columns=df.columns), meta=df[categorical_columns]._meta)\n",
    "\n",
    "# Convert categorical columns to categorical dtype\n",
    "df[categorical_columns] = df[categorical_columns].categorize()\n",
    "\n",
    "# Encode categorical variables\n",
    "encoder = DummyEncoder()\n",
    "df = encoder.fit_transform(df)\n",
    "\n",
    "# Scale numeric features in parallel\n",
    "df[numeric_columns] = df[numeric_columns].map_partitions(lambda df: pd.DataFrame(scaler.transform(df), columns=df.columns), meta=df[numeric_columns]._meta)\n"
   ],
   "id": "4b8d523605ca2a63",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://127.0.0.1:54186' processes=4 threads=8, memory=15.69 GiB>\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T18:59:19.990280Z",
     "start_time": "2024-06-25T18:59:19.976497Z"
    }
   },
   "cell_type": "code",
   "source": "print(df.columns)",
   "id": "25236cc29eb5f9e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['isFraud', 'TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3',\n",
      "       'card5', 'dist1', 'dist2', 'ProductCD_C', 'ProductCD_H', 'ProductCD_R',\n",
      "       'ProductCD_S', 'ProductCD_W', 'card4_american express',\n",
      "       'card4_discover', 'card4_mastercard', 'card4_visa', 'card6_charge card',\n",
      "       'card6_credit', 'card6_debit', 'card6_debit or credit'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T19:00:07.096147Z",
     "start_time": "2024-06-25T19:00:05.764621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dask.distributed import get_worker\n",
    "from pymongo import MongoClient\n",
    "\n",
    "def get_mongo_client():\n",
    "    worker = get_worker()  # This raises ValueError if not running within a worker context\n",
    "    if not hasattr(worker, 'mongo_client'):\n",
    "        connection_string = uri\n",
    "        worker.mongo_client = MongoClient(connection_string)\n",
    "    return worker.mongo_client\n",
    "\n",
    "def insert_into_mongo(df_part):\n",
    "    try:\n",
    "        client = get_mongo_client()\n",
    "        db = client.fraud_db\n",
    "        transactions_collection = db.transactions\n",
    "        records = df_part.to_dict(orient='records')\n",
    "        if records:\n",
    "            transactions_collection.insert_many(records)\n",
    "    except ValueError:\n",
    "        print(\"Not running on a worker. Proper MongoDB operations can't be performed.\")\n",
    "\n",
    "def load_data_from_mongo(uri, db_name, collection_name):\n",
    "    client = MongoClient(uri)\n",
    "    db = client[db_name]\n",
    "    collection = db[collection_name]\n",
    "    cursor = collection.find({})\n",
    "\n",
    "    # Convert cursor to DataFrame directly if memory allows\n",
    "    return pd.DataFrame(list(cursor))\n"
   ],
   "id": "9fa5d8551a253c55",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T18:51:38.664119Z",
     "start_time": "2024-06-25T18:49:37.007283Z"
    }
   },
   "cell_type": "code",
   "source": "df.map_partitions(insert_into_mongo, meta=int).compute()\n",
   "id": "c5b748e10d0e3e93",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SvenEggers\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dask_expr\\_expr.py:4011: FutureWarning: Meta is not valid, `map_partitions` and `map_overlap` expects output to be a pandas object. Try passing a pandas object as meta or a dict or tuple representing the (name, dtype) of the columns. In the future the meta you passed will not work.\n",
      "  warnings.warn(\n",
      "C:\\Users\\SvenEggers\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dask_expr\\_expr.py:4011: FutureWarning: Meta is not valid, `map_partitions` and `map_overlap` expects output to be a pandas object. Try passing a pandas object as meta or a dict or tuple representing the (name, dtype) of the columns. In the future the meta you passed will not work.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    None\n",
       "1    None\n",
       "2    None\n",
       "3    None\n",
       "4    None\n",
       "5    None\n",
       "6    None\n",
       "7    None\n",
       "8    None\n",
       "9    None\n",
       "dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T18:51:45.383201Z",
     "start_time": "2024-06-25T18:51:45.079029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Fetch Data from MongoDB\n",
    "#from dask.delayed import delayed\n",
    "#chunks = load_data_from_mongo(uri, db_name, collection_name, chunk_size=10000)\n",
    "\n",
    "# Create Dask DataFrame from delayed chunks\n",
    "#ddf = dd.from_delayed([delayed(pd.DataFrame)(chunk) for chunk in chunks])"
   ],
   "id": "6abd5e76d6a1048a",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T19:03:45.350918Z",
     "start_time": "2024-06-25T19:00:15.116695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert to Dask array\n",
    "X = df.drop('isFraud', axis=1).to_dask_array(lengths=True)\n",
    "y = df['isFraud'].to_dask_array(lengths=True)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Initialize the model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Use ParallelPostFit to work with Dask\n",
    "model = ParallelPostFit(estimator=rf)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Compute the classification report\n",
    "report = classification_report(y_test.compute(), y_pred.compute())\n",
    "\n",
    "print(report)"
   ],
   "id": "928ef7f96fc66bf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SvenEggers\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\distributed\\client.py:3164: UserWarning: Sending large graph of size 302.11 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99    113931\n",
      "           1       0.85      0.45      0.58      4182\n",
      "\n",
      "    accuracy                           0.98    118113\n",
      "   macro avg       0.91      0.72      0.79    118113\n",
      "weighted avg       0.98      0.98      0.97    118113\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T19:45:07.053437Z",
     "start_time": "2024-06-25T19:45:07.043040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Predict isFraud on test dataset\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Load the test dataset\n",
    "test_df = dd.read_csv('test_transaction.csv', usecols=['TransactionID'] + selected_columns[1:], dtype=dtypes).set_index('TransactionID')\n",
    "\n",
    "print(test_df.columns)"
   ],
   "id": "a262567b9bee542b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2',\n",
      "       'card3', 'card4', 'card5', 'card6', 'dist1', 'dist2'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T19:45:18.923793Z",
     "start_time": "2024-06-25T19:45:13.711569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Identify numeric and categorical columns\n",
    "numeric_columns_test = ['TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'dist1', 'dist2']\n",
    "categorical_columns_test = ['ProductCD', 'card4', 'card6']\n",
    "\n",
    "# Handle missing values for numeric columns\n",
    "test_df[numeric_columns_test] = test_df[numeric_columns_test].map_partitions(lambda df: imputer_numeric.fit_transform(df), meta=test_df[numeric_columns_test]._meta)\n",
    "\n",
    "# Handle missing values for categorical columns\n",
    "test_df[categorical_columns_test] = test_df[categorical_columns_test].map_partitions(lambda df: imputer_categorical.fit_transform(df), meta=test_df[categorical_columns_test]._meta)\n",
    "\n",
    "# Convert categorical columns to categorical dtype\n",
    "test_df[categorical_columns_test] = test_df[categorical_columns_test].categorize()\n",
    "\n",
    "# Encode categorical variables\n",
    "encoder_test = DummyEncoder()\n",
    "test_df = encoder_test.fit_transform(test_df)\n",
    "\n",
    "# Take a small sample to fit the scaler\n",
    "sample_test = test_df.sample(frac=0.1, random_state=42).compute()\n",
    "\n",
    "# Fit the scaler on the sample\n",
    "scaler_test = StandardScaler().fit(sample_test[numeric_columns_test])\n",
    "\n",
    "# Scale numeric features in parallel\n",
    "test_df[numeric_columns_test] = test_df[numeric_columns_test].map_partitions(lambda df: pd.DataFrame(scaler_test.transform(df), columns=df.columns), meta=test_df[numeric_columns_test]._meta)\n",
    "\n",
    "# Ensure the missing 'card6_debit or credit' column is added with zeros\n",
    "if 'card6_debit or credit' not in test_df.columns:\n",
    "    test_df['card6_debit or credit'] = 0\n"
   ],
   "id": "5a48e813191dcfac",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T19:45:21.602190Z",
     "start_time": "2024-06-25T19:45:21.593682Z"
    }
   },
   "cell_type": "code",
   "source": "print(test_df.columns)",
   "id": "553f54c876a6181a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5',\n",
      "       'dist1', 'dist2', 'ProductCD_C', 'ProductCD_H', 'ProductCD_R',\n",
      "       'ProductCD_S', 'ProductCD_W', 'card4_american express',\n",
      "       'card4_discover', 'card4_mastercard', 'card4_visa', 'card6_charge card',\n",
      "       'card6_credit', 'card6_debit', 'card6_debit or credit'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T19:51:40.986181Z",
     "start_time": "2024-06-25T19:51:27.261755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Predict isFraud on the test dataset\n",
    "test_predictions = model.predict(test_df)\n",
    "\n",
    "# Define the path to save the new submission file\n",
    "submission_file_path = r'C:\\Users\\SvenEggers\\.kaggle\\sample_submission_1.csv' #REPLACE WITH YOUR WORKING TRAJECTORY\n",
    "\n",
    "# Load the sample submission file to get the TransactionID\n",
    "submission_df = dd.read_csv('sample_submission.csv').set_index('TransactionID')\n",
    "\n",
    "# Convert predictions to a Dask DataFrame and align with the submission DataFrame\n",
    "predictions_df = dd.from_pandas(pd.DataFrame({\n",
    "    'TransactionID': test_df.index.compute(),\n",
    "    'isFraud': test_predictions.compute()\n",
    "}), npartitions=1).set_index('TransactionID')\n",
    "\n",
    "# Merge the predictions with the submission file\n",
    "submission_df = submission_df.drop(columns='isFraud', errors='ignore')  # Drop existing isFraud if present\n",
    "submission_df = submission_df.merge(predictions_df, left_index=True, right_index=True, how='left')\n",
    "\n",
    "# Save the updated submission file, replacing if it exists\n",
    "submission_df.to_csv(submission_file_path, single_file=True)\n",
    "\n",
    "print(f'Submission file saved to {submission_file_path}')"
   ],
   "id": "3ee1147f752b2f98",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SvenEggers\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\SvenEggers\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\SvenEggers\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\distributed\\client.py:3164: UserWarning: Sending large graph of size 302.10 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved to C:\\Users\\SvenEggers\\.kaggle\\sample_submission_1.csv\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T19:59:31.137725Z",
     "start_time": "2024-06-25T19:59:30.162098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Count the numbers of isFraud in the submission\n",
    "\n",
    "# Load the sample submission dataset\n",
    "sample_submission_df = pd.read_csv('sample_submission_1.csv')\n",
    "\n",
    "# Check the first few rows to ensure it loaded correctly\n",
    "print(sample_submission_df.head())\n",
    "\n",
    "# Count the number of 1s in the 'isFraud' column\n",
    "is_fraud_count = sample_submission_df['isFraud'].sum()\n",
    "\n",
    "print(f\"The number of 1s in the 'isFraud' column: {is_fraud_count}\")"
   ],
   "id": "c10df3b9437a409f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TransactionID  isFraud\n",
      "0        3663549        0\n",
      "1        3663550        0\n",
      "2        3663551        0\n",
      "3        3663552        0\n",
      "4        3663553        0\n",
      "The number of 1s in the 'isFraud' column: 1148\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cfe73e75df9199b6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

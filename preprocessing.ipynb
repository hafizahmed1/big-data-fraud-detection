{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-18T21:02:32.534320Z",
     "start_time": "2024-06-18T21:02:14.680930Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Path to your CSV file\n",
    "file_path = r'C:\\Users\\ahmad\\PycharmProjects\\big-data-fraud-detection\\ieee-fraud-detection\\train_transaction.csv'\n",
    "\n",
    "\n",
    "# Use pandas to read in a small portion of the file to infer types\n",
    "sample = pd.read_csv(file_path, nrows=100)\n",
    "\n",
    "# Get the inferred data types\n",
    "dtype_dict = sample.dtypes.apply(lambda x: x.name).to_dict()\n",
    "\n",
    "# Now read the full dataset with Dask using these types\n",
    "ddf = dd.read_csv(file_path, dtype=dtype_dict)\n",
    "\n",
    "# Display the first few rows to ensure it loads correctly\n",
    "print(ddf.head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TransactionID  isFraud  TransactionDT  TransactionAmt ProductCD  card1  \\\n",
      "0        2987000        0          86400            68.5         W  13926   \n",
      "1        2987001        0          86401            29.0         W   2755   \n",
      "2        2987002        0          86469            59.0         W   4663   \n",
      "3        2987003        0          86499            50.0         W  18132   \n",
      "4        2987004        0          86506            50.0         H   4497   \n",
      "\n",
      "   card2  card3       card4  card5  ... V330  V331  V332  V333  V334 V335  \\\n",
      "0    NaN  150.0    discover  142.0  ...  NaN   NaN   NaN   NaN   NaN  NaN   \n",
      "1  404.0  150.0  mastercard  102.0  ...  NaN   NaN   NaN   NaN   NaN  NaN   \n",
      "2  490.0  150.0        visa  166.0  ...  NaN   NaN   NaN   NaN   NaN  NaN   \n",
      "3  567.0  150.0  mastercard  117.0  ...  NaN   NaN   NaN   NaN   NaN  NaN   \n",
      "4  514.0  150.0  mastercard  102.0  ...  0.0   0.0   0.0   0.0   0.0  0.0   \n",
      "\n",
      "  V336  V337  V338  V339  \n",
      "0  NaN   NaN   NaN   NaN  \n",
      "1  NaN   NaN   NaN   NaN  \n",
      "2  NaN   NaN   NaN   NaN  \n",
      "3  NaN   NaN   NaN   NaN  \n",
      "4  0.0   0.0   0.0   0.0  \n",
      "\n",
      "[5 rows x 394 columns]\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T11:24:05.764673Z",
     "start_time": "2024-06-19T11:23:59.223163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=2, threads_per_worker=2)"
   ],
   "id": "b3a0a20b71ba8eaf",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T21:02:59.484232Z",
     "start_time": "2024-06-18T21:02:59.451292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "info = ddf.info()\n",
    "print(info)"
   ],
   "id": "fdd68068a120f24b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask_expr.DataFrame'>\n",
      "Columns: 394 entries, TransactionID to V339\n",
      "dtypes: float64(376), int64(4), string(14)None\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T21:06:44.492320Z",
     "start_time": "2024-06-18T21:03:02.561582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Calculate the count of nulls for each column\n",
    "null_counts = ddf.isnull().sum().compute()\n",
    "\n",
    "# Calculate the total number of entries in the DataFrame\n",
    "total_entries = len(ddf)\n",
    "\n",
    "# Set a threshold for missing values (e.g., columns with more than 50% missing values)\n",
    "threshold_percentage = 50\n",
    "threshold = total_entries * (threshold_percentage / 100)\n",
    "\n",
    "# Identify columns that have missing values above the threshold\n",
    "columns_to_drop = null_counts[null_counts > threshold].index\n",
    "print(columns_to_drop)\n"
   ],
   "id": "acbfca6929379b7c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['dist1', 'dist2', 'R_emaildomain', 'D5', 'D6', 'D7', 'D8', 'D9', 'D12',\n",
      "       'D13',\n",
      "       ...\n",
      "       'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338',\n",
      "       'V339'],\n",
      "      dtype='object', length=174)\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T21:11:39.014257Z",
     "start_time": "2024-06-18T21:11:38.834818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Drop these columns from the DataFrame\n",
    "ddf = ddf.drop(columns=columns_to_drop)  # Drop columns and reassign"
   ],
   "id": "1a1e31a922cc77b8",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T21:11:44.945102Z",
     "start_time": "2024-06-18T21:11:43.756784Z"
    }
   },
   "cell_type": "code",
   "source": "ddf=ddf.persist()",
   "id": "e3c33456f101cc63",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T21:11:51.755470Z",
     "start_time": "2024-06-18T21:11:48.041418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Identify numerical and categorical columns\n",
    "numerical_cols = ddf.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = [col for col in ddf.columns if str(ddf.dtypes[col]) in ['object', 'category', 'string', 'string[pyarrow]']]\n",
    "\n",
    "for col in categorical_cols:\n",
    "    ddf[col] = ddf[col].astype('string')\n",
    "print(f\"Columns numerical_cols: {len(numerical_cols)}\")\n",
    "print(f\"Columns categorical_cols: {len(categorical_cols)}\")"
   ],
   "id": "4886439cc4608a6f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns numerical_cols: 211\n",
      "Columns categorical_cols: 9\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T21:25:15.960303Z",
     "start_time": "2024-06-18T21:12:24.712177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fill missing values for numerical columns with the mean of each column\n",
    "for col in numerical_cols:\n",
    "    # Compute the mean of the column (use .compute() to execute the calculation)\n",
    "    mean_value = ddf[col].mean().compute()\n",
    "    ddf[col] = ddf[col].fillna(mean_value)\n",
    "\n",
    "# Fill missing values for categorical columns with the mode of each column\n",
    "for col in categorical_cols:\n",
    "    # Compute the mode. Note: Dask's mode computation might be less straightforward than pandas,\n",
    "    # often requiring a workaround to get the first mode value because Dask might return multiple modes.\n",
    "    mode_value = ddf[col].dropna().value_counts().idxmax().compute()\n",
    "    ddf[col] = ddf[col].fillna(mode_value)"
   ],
   "id": "5f4493a5abc70ca1",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T21:30:53.212450Z",
     "start_time": "2024-06-18T21:27:57.097262Z"
    }
   },
   "cell_type": "code",
   "source": "ddf=ddf.persist()",
   "id": "1763b9f6f8806b7f",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T21:34:45.852289Z",
     "start_time": "2024-06-18T21:34:38.404135Z"
    }
   },
   "cell_type": "code",
   "source": "print(ddf.isnull().any().any().compute())\n",
   "id": "8e991dfcf08f639a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T22:28:57.009185Z",
     "start_time": "2024-06-18T22:24:05.309933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Repartition to a single partition\n",
    "ddf = ddf.repartition(npartitions=1)\n",
    "\n",
    "# Save to CSV\n",
    "ddf.to_csv('cleaned_transactions.csv', single_file=True, index=False)"
   ],
   "id": "8c4027fc51055923",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\ahmad\\\\PycharmProjects\\\\big-data-fraud-detection\\\\cleaned_transactions.csv']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T20:32:45.095185Z",
     "start_time": "2024-06-19T20:32:33.451779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=2, threads_per_worker=2)"
   ],
   "id": "ed2c84a844e1cda3",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T20:32:48.900957Z",
     "start_time": "2024-06-19T20:32:45.098725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "ddf=dd.read_csv('cleaned_transactions.csv')"
   ],
   "id": "6801175e64219187",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T20:32:49.947881Z",
     "start_time": "2024-06-19T20:32:49.863211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Identify numerical and categorical columns\n",
    "numerical_cols = ddf.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "categorical_cols = [col for col in ddf.columns if str(ddf.dtypes[col]) in ['object', 'category', 'string', 'string[pyarrow]']]\n"
   ],
   "id": "d52c3ced9cf3e3c5",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T20:32:52.820080Z",
     "start_time": "2024-06-19T20:32:52.684044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for col in categorical_cols:\n",
    "    ddf[col] = ddf[col].astype('string')\n",
    "print(f\"Columns numerical_cols: {numerical_cols}\")\n",
    "print(f\"Columns categorical_cols: {categorical_cols}\")"
   ],
   "id": "40ac98c0778868aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns numerical_cols: Index(['TransactionID', 'isFraud', 'TransactionDT', 'TransactionAmt', 'card1',\n",
      "       'card2', 'card3', 'card5', 'addr1', 'addr2',\n",
      "       ...\n",
      "       'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320',\n",
      "       'V321'],\n",
      "      dtype='object', length=211)\n",
      "Columns categorical_cols: ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'M1', 'M2', 'M3', 'M4', 'M6']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T20:33:24.805353Z",
     "start_time": "2024-06-19T20:32:56.540984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Using Dask to perform one-hot encoding\n",
    "ddf = ddf.categorize(columns=categorical_cols)  # Converts columns to categorical dtype\n",
    "ddf = dd.get_dummies(ddf, columns=categorical_cols)  # Perform one-hot encoding\n"
   ],
   "id": "3f93d1669deaaa92",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T20:33:24.821739Z",
     "start_time": "2024-06-19T20:33:24.805880Z"
    }
   },
   "cell_type": "code",
   "source": "transactionIDs=ddf['TransactionID']",
   "id": "e0542b01202be303",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T20:33:28.822320Z",
     "start_time": "2024-06-19T20:33:24.821739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dask_ml.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "def scale_partition(partition):\n",
    "    # Scale only the numerical columns\n",
    "    partition[numerical_cols] = scaler.fit_transform(partition[numerical_cols])\n",
    "    return partition\n",
    "\n",
    "# Apply the scaler to each partition\n",
    "ddf = ddf.map_partitions(scale_partition)\n",
    "\n",
    "#persist the DataFrame to optimize further computations\n",
    "ddf = ddf.persist()\n"
   ],
   "id": "aee79b6f1c7ab5ca",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T20:34:11.147371Z",
     "start_time": "2024-06-19T20:34:10.718062Z"
    }
   },
   "cell_type": "code",
   "source": "print(ddf.head())",
   "id": "10c31c535dc00c65",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TransactionID  isFraud  TransactionDT  TransactionAmt     card1     card2  \\\n",
      "0       0.000000      0.0   0.000000e+00        0.014123  0.743158  0.525111   \n",
      "1       0.000022      0.0   9.840069e-07        0.005944  0.100851  0.608000   \n",
      "2       0.000044      0.0   6.789648e-05        0.012156  0.210557  0.780000   \n",
      "3       0.000065      0.0   9.741669e-05        0.010292  0.984993  0.934000   \n",
      "4       0.000087      0.0   1.043047e-04        0.010292  0.201012  0.828000   \n",
      "\n",
      "      card3     card5     addr1     addr2  ...  M1_T   M2_F  M2_T   M3_F  \\\n",
      "0  0.387597  0.306569  0.488636  0.831461  ...  True  False  True  False   \n",
      "1  0.387597  0.014599  0.511364  0.831461  ...  True  False  True  False   \n",
      "2  0.387597  0.481752  0.522727  0.831461  ...  True  False  True  False   \n",
      "3  0.387597  0.124088  0.854545  0.831461  ...  True  False  True  False   \n",
      "4  0.387597  0.014599  0.727273  0.831461  ...  True  False  True  False   \n",
      "\n",
      "   M3_T  M4_M0  M4_M1  M4_M2   M6_F   M6_T  \n",
      "0  True  False  False   True  False   True  \n",
      "1  True   True  False  False  False   True  \n",
      "2  True   True  False  False   True  False  \n",
      "3  True   True  False  False   True  False  \n",
      "4  True   True  False  False   True  False  \n",
      "\n",
      "[5 rows x 294 columns]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T20:50:09.924631Z",
     "start_time": "2024-06-19T20:37:35.516005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from dask_ml.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X = ddf.drop(['isFraud', 'TransactionID'], axis=1)\n",
    "y = ddf['isFraud']\n",
    "\n",
    "# Split data using dask_ml's train_test_split which handles Dask DataFrames\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Compute the Dask DataFrames to pandas DataFrames and create a copy\n",
    "X_train_computed = X_train.compute().copy()\n",
    "y_train_computed = y_train.compute().copy()\n",
    "\n",
    "X_test_computed=X_test.compute().copy()\n",
    "y_test_computed=y_test.compute().copy()\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "model.fit(X_train_computed, y_train_computed)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_computed)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_computed, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test_computed, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test_computed, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test_computed, y_pred))\n"
   ],
   "id": "64761b7497ee80df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n",
      "Recall: 0.45396825396825397\n",
      "Precision: 0.9426977687626775\n",
      "F1 Score: 0.6128234712378441\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T20:51:57.224904Z",
     "start_time": "2024-06-19T20:51:57.006373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get feature importances\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to view the importances more easily\n",
    "import pandas as pd\n",
    "\n",
    "features = X_train.columns\n",
    "importance_df = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(importance_df.head(20))"
   ],
   "id": "321efb86f71a4df4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Feature  Importance\n",
      "1    TransactionAmt    0.038369\n",
      "0     TransactionDT    0.037728\n",
      "8                C1    0.037542\n",
      "2             card1    0.032813\n",
      "3             card2    0.026406\n",
      "20              C13    0.025772\n",
      "21              C14    0.024442\n",
      "6             addr1    0.021865\n",
      "9                C2    0.019567\n",
      "18              C11    0.019169\n",
      "11               C4    0.017112\n",
      "13               C6    0.016210\n",
      "15               C8    0.014424\n",
      "5             card5    0.013847\n",
      "73              V45    0.013162\n",
      "115             V87    0.012994\n",
      "114             V86    0.010787\n",
      "19              C12    0.010691\n",
      "28              D15    0.010215\n",
      "23               D2    0.009746\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T20:52:39.089842Z",
     "start_time": "2024-06-19T20:52:39.032197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Select the top N most important features, for example top 50\n",
    "top_features = importance_df['Feature'][:50]\n",
    "print(top_features.values)"
   ],
   "id": "3eb37d3baddc4cd3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TransactionAmt' 'TransactionDT' 'C1' 'card1' 'card2' 'C13' 'C14' 'addr1'\n",
      " 'C2' 'C11' 'C4' 'C6' 'C8' 'card5' 'V45' 'V87' 'V86' 'C12' 'D15' 'D2'\n",
      " 'C10' 'D1' 'D4' 'V44' 'V307' 'P_emaildomain_gmail.com' 'D10' 'V283'\n",
      " 'card3' 'C7' 'V310' 'V308' 'D11' 'V317' 'D3' 'V23' 'V282' 'C9' 'V315'\n",
      " 'V38' 'V127' 'V24' 'V285' 'V314' 'V313' 'card6_debit' 'V62' 'V78' 'V312'\n",
      " 'V83']\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "97ee9f4b612f8b0c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
